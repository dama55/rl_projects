# 強化学習学習プロジェクト
![マリオgif画像](/docs/img/mario_chkpt_141.gif)

<br />

## プロジェクトの概要

目的はPytorchを利用した強化学習でよく使われる手法を複数環境で試すことで，強化学習の一般的な流れを知り，利点と欠点を知ること．
環境として"Mountain Car", "CartPole", "Mario"．強化学習手法は"PPO"，"DQN"，その他（モデルベース手法，アテンションを追加したDQNなどを入れたい）．

<br />

## プロジェクトを開発した背景

AIが高度な知能を獲得させる研究の最近のブームとして，エージェントが環境と相互作用して能動的に探索することが大切だという考え方がある．
エージェントは常に報酬が高い行動をするのみではなく，どのような行動を起こしたら，環境はどのように動くのかのイメージを自ら作り出す必要がある．
その一環としてDreamer v3のようなモデルベース強化学習や行動を自由エネルギー最小化の一部として組み込む，自由エネルギー原理などが注目されている．
一方，既に実世界のロボットに広く応用されている手法として，DQNのような報酬の最大化と探索の両方を実行することで，ポリシー（行動の選択確率分布を表す確率密度関数）や価値関数（各状態や状態・行動が将来的に得ると期待される報酬の累積和を表す関数）を学習する一般的な強化学習手法がある．
本プロジェクトでは，強化学習の一般的な処理の流れや効果を知ることで，強化学習でできること，できないことを学ぶことで，強化学習のみで既にできることと，モデルベース強化学習や自由エネルギー原理に何が求められるのかを明確にすることを目的とする．


<br />


## 環境とアルゴリズム

### 環境
| [CartPole]() |
| ---- |
|![CartPole](https://gymnasium.farama.org/_images/cart_pole.gif)|
|棒を横に倒さないようにする環境|


| [Mario](https://opendilab.github.io/DI-engine/13_envs/gym_super_mario_bros.html) |
| ---- |
| ![Mario](https://opendilab.github.io/DI-engine/_images/mario.png)|
| スーパーマリオブラザーズのマリオを操作してゴールさせる環境|

| [Mountain Car](https://www.gymlibrary.dev/environments/classic_control/mountain_car/) |
| ---- |
| ![MountainCar](https://www.gymlibrary.dev/_images/mountain_car.gif)|
| 乗り物を左右に移動して山を上る環境|



<br />

## 使用技術
* Python3.8.0
* pip 24.0
依存関係は `requirements.txt` を参照．


<br />

## ファイル構成

```
📂 my_ppo
├── 📂 Agents
├── 📂 Base
├── 📂 Envs
├── 📂 Experiments
├── 📂 Trainers
└── 📂 Util
```


### 📂 Agents
DQN,PPOなどの機械学習エージェントを含むファイル．ネットワークモデルと学習処理を実装している．

### 📂 Base
環境，エージェント，ロガー等の抽象クラスを定義．

### 📂 Envs
エージェントの環境を実装．

### 📂 Experiments
Trainerを利用して実際の実験を記述．

### 📂 Trainers
各実験設定ごとに環境とエージェントを呼び出して，実行するためのクラスを定義．

### 📂 Util
各種コードで共通で必要となる処理をまとめる．


<br />

## インストールと実行

（作成中）

## 今後の展望

本プロジェクトは現状PPOやDQNを個別に実装したものの，それらを同じ処理の枠組みで扱えるようなクラスの定義が未完成．残件は以下の通り．

* TensorDictを利用した処理の高速化を試す．
* TensorDictを利用したエージェント，ロガーの抽象クラスを定義する．
* 抽象クラスをもとにPPO,DQNなどの具体的なクラスのAgents,Trainers,Experimentsを追加．
* 追加の環境，エージェントについての実装も加える．
* 本プロジェクトの目的である，強化学習の利点と欠点をまとめる．